{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2 logistic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Methods for doing logistic regression.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from utils import sigmoid\n",
    "\n",
    "def logistic_predict(weights, data):\n",
    "    \"\"\"\n",
    "    Compute the probabilities predicted by the logistic classifier.\n",
    "\n",
    "    Note: N is the number of examples and\n",
    "          M is the number of features per example.\n",
    "\n",
    "    Inputs:\n",
    "        weights:    (M+1) x 1 vector of weights, where the last element\n",
    "                    corresponds to the bias (intercepts).\n",
    "        data:       N x M data matrix where each row corresponds\n",
    "                    to one data point.\n",
    "    Outputs:\n",
    "        y:          :N x 1 vector of probabilities. This is the output of the classifier.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function\n",
    "    N = data.shape[0]\n",
    "    # append vector contains only 1s\n",
    "    data = np.concatenate((data, np.full((N, 1), 1)), axis=1)\n",
    "\n",
    "    # predict the output given weights\n",
    "    y = sigmoid(np.matmul(data, weights))\n",
    "\n",
    "    return y\n",
    "\n",
    "def evaluate(targets, y):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    Inputs:\n",
    "        targets : N x 1 vector of targets.\n",
    "        y       : N x 1 vector of probabilities.\n",
    "    Outputs:\n",
    "        ce           : (scalar) Cross entropy. CE(p, q) = E_p[-log q]. Here we want to compute CE(targets, y)\n",
    "        frac_correct : (scalar) Fraction of inputs classified correctly.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function\n",
    "    # Squeeze 2d column or row vector into 1d array\n",
    "    targets = np.squeeze(targets)\n",
    "    y = np.squeeze(y)\n",
    "    # Calculate cross entropy loss\n",
    "    ce = -(np.dot(targets, np.log(y + 1e-8)) + np.dot(1 - targets, np.log(1 - y + 1e-8)))\n",
    "\n",
    "    # Calculate the percentage of correct prediction with 0.5 as threshold\n",
    "    correct_num = np.sum(np.absolute(y - targets) <= .5)\n",
    "    frac_correct = correct_num / float(targets.shape[0])\n",
    "\n",
    "    return ce, frac_correct\n",
    "\n",
    "def logistic(weights, data, targets, hyperparameters):\n",
    "    \"\"\"\n",
    "    Calculate negative log likelihood and its derivatives with respect to weights.\n",
    "    Also return the predictions.\n",
    "\n",
    "    Note: N is the number of examples and\n",
    "          M is the number of features per example.\n",
    "\n",
    "    Inputs:\n",
    "        weights:    (M+1) x 1 vector of weights, where the last element\n",
    "                    corresponds to bias (intercepts).\n",
    "        data:       N x M data matrix where each row corresponds\n",
    "                    to one data point.\n",
    "        targets:    N x 1 vector of targets class probabilities.\n",
    "        hyperparameters: The hyperparameters dictionary.\n",
    "\n",
    "    Outputs:\n",
    "        f:       The sum of the loss over all data points. This is the objective that we want to minimize.\n",
    "        df:      (M+1) x 1 vector of derivative of f w.r.t. weights.\n",
    "        y:       N x 1 vector of probabilities.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function\n",
    "    no_penalize = hyperparameters.copy()\n",
    "    no_penalize[\"weight_regularization\"] = 0\n",
    "    return logistic_pen(weights, data, targets, no_penalize)\n",
    "\n",
    "\n",
    "def logistic_pen(weights, data, targets, hyperparameters):\n",
    "    \"\"\"\n",
    "    Calculate negative log likelihood and its derivatives with respect to weights.\n",
    "    Also return the predictions.\n",
    "\n",
    "    Note: N is the number of examples and\n",
    "          M is the number of features per example.\n",
    "\n",
    "    Inputs:\n",
    "        weights:    (M+1) x 1 vector of weights, where the last element\n",
    "                    corresponds to bias (intercepts).\n",
    "        data:       N x M data matrix where each row corresponds\n",
    "                    to one data point.\n",
    "        targets:    N x 1 vector of targets class probabilities.\n",
    "        hyperparameters: The hyperparameters dictionary.\n",
    "\n",
    "    Outputs:\n",
    "        f:             The sum of the loss over all data points. This is the objective that we want to minimize.\n",
    "        df:            (M+1) x 1 vector of derivative of f w.r.t. weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Finish this function\n",
    "    y = logistic_predict(weights, data)\n",
    "\n",
    "    # Calculate the difference between the prediction and true value\n",
    "    diff = targets - y\n",
    "\n",
    "    # Compute gradient of dL/dw, and change to column vector\n",
    "    df = (np.matmul(diff.T, data)).T\n",
    "\n",
    "    # Add derivative of bias to the last column\n",
    "    df = np.append(df, [[np.sum(diff)]], axis=0)\n",
    "    df = -df\n",
    "\n",
    "    # Derived the regulizer and add to the derivative\n",
    "    df[:-1] += hyperparameters[\"weight_regularization\"] * weights[:-1] * target.shape[0]\n",
    "    # Compute cross entropy loss\n",
    "    f, _ = evaluate(targets, y)\n",
    "\n",
    "    # Include the regulizer\n",
    "    f += (hyperparameters[\"weight_regularization\"] / 2) * np.sum(weights**2) * target.shape[0]\n",
    "    return f, df, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
